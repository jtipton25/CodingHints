##
## username: jtipton
##
## pwd: same as login for stats computers... <- for me only
##

put filepath/filename  # copy from local directory to sftp directory
exit  # to log-off the ssh


##
## Tutorial for logging into the CSU CRAY Supercomputer
##

##
## hostname of the CRAY: cray2.colostate.edu
##

##
## IP address of the CRAY: 129.82.103.183
##

## 
## To login to the CRAY with ssh in a terminal window
##

ssh username@cray2.colostate.edu # where username is your CRAY username

##
## To transfer files to/from the CRAY with sftp 
##

sftp username@cray2.colostate.edu

##
## On a windows machine you can use PuTTy or FileZilla
##
## PuTTy is available at: http://www.chiark.greenend.org.uk/~sgtatham/putty/
##
## FileZilla is available at: http://filezilla-project.org/
##

##
## To create the bash.profile on the CRAY
##
## Type

vi bash.profile
## in the editor type: <- use the most recent version on R on the CRAY, check this
## using ls/apps

export PATH=/apps/R-2.14.14/bin:$PATH
export TMP=$HOME/lustrefs/tmp/
## Press the escape key and type

:wq
## then press the enter key

##
## The CRAY is organized into modules that configure the shell for a particular
## application. The advantage of modules is that you are not required to
## specify explicit paths for different executable versions or to set the
## $MANPATH and other environment variables manually.
## 

module list  # view the module files currently laoded in your shell environment

module avail  # see a list of all module files on the CRAY

module load modulefile  # to load a modulefile

module unload modulefile  # unloads a loaded modulefile

module swap modulefile1 modulefile2 # to swap modulefile1 for modulefile2
                                    #e.g. module swap PrgEnv-cray PrgEnv-gnu
module show modulefile  # see the contents of a particular module

module help modulefile  # to see a description of the modulefile

##
## Interactive Jobs
##

# All parallel compute jobs must be run in the "lustrefs" directory

# Small jobs can be run on the Interactive Nodes to test code, etc.

# Large parallel jobs should be run on the Batch Compute Nodes 

##
## aprun Submits jobs for placement and execution
##

aprun -n 1 executable  # will run the executable file
                       # the -n 1 specifies the use of 1 core

aprun-n 24 executable  # run an executable file over 24 cores

apstat  # Shows all currently running applications

apkill apid  # Kills a curently running interactive job
             # apid is the application ID found by running the apstat command
             # you must be the owner of an application to kill it

##
## Batch Jobs
##

# Small queue - reserved for small jobs that can run in less than one hour
# Medium queue - reserved for jobs that run in less than one day
# Large queue - reserved for jobs that run in less than one week
# Priority queue - reserved for special uses <- conatact system admin

## Max number of jobs per user
# Small queue - 20
# Medium queue - 2
# Large queue - 1

qstat -Q  # List all available queues (brief)

qstat -Qf  # List all available queues (full)

qstat  # show the status of jobs in all queues

qstat -u jtipton  # show only the status of jobs that belong in your account

qsub filename  # Submit a job to the default batch queue where filename 
               # is the name of a file that contains batch queue commands

qdel jobid  # Delte a job from the batch queues where jobid is the job ID 
            # displayed by the qstat command

##
## Sample Batch Jobs
##

# To use the batch queues, you must create a text file (batch script) that 
# contains Torqu/PBS commands. You submit this file with the qsub command:

qsub filename

# Here is a sample batch script that you can use to run jobs on the Cray, or 
# use as a template for more complex script

  #!/bin/bash 
  #PBS œôòóN jobname 
  #PBS œôòój oe 
  #PBS œôòól mppwidth=24 
  #PBS œôòól walltime=1:00:00 
  #PBS œôòóq small 
  cd $PBS_O_WORKDIR 
  aprun œôòón 24 executable
# The first line specifies the shell environment to use for the batch job. The
# "#PBS" mneumonic represents a PBS directive. The œôòü-Nœôòý directive renames the
# output files to whatever name you specify. The œôòü-j oeœôòý option indicates that
# standard output and standard error information should be combined in a single
# file. The œôòü-l mppwidthœôòý option specifies the number of cores to allocate to
# your job. The number of cores should not exceed 1,344, which is the maximum
# number of cores available in the largest batch queue. The œôòü-l walltimeœôòý
# option specifies the maximum amount of time in hours, minutes and seconds
# that the job may run before exiting. The œôòü-q smallœôòý option specifies which
# queue the job should be run in. There are four options: œôòüsmallœôòý, œôòümediumœôòý, 
# œôòülargeœôòý, œôòüccm_queueœôòý. If no queue is specified in the batch script, the 
# job will be routed to the œôòüsmallœôòý queue by default. The PBS_O_WORKDIR 
# environment variable is generated by Torque/PBS and contains the absolute 
# path to the directory from which you submitted your job. This is required 
# for Torque/PBS to find your executable files.

##
## CRAY supports either OpenMP or MPI to parallelize code
##