;; This buffer is for notes you don't want to save, and for Lisp evaluation.
;; If you want to create a file, visit that file with C-x C-f,
;; then enter the text in that file's own buffer.
##
## username: jtipton
##
## pwd: same as login for stats computers... <- for me only
##


##
## Tutorial for logging into the CSU CRAY Supercomputer
##

##
## hostname of the CRAY: cray2.colostate.edu
##

##
## IP address of the CRAY: 129.82.103.183
##

## 
## To login to the CRAY with ssh in a terminal window
##

ssh username@cray2.colostate.edu # where username is your CRAY username

##
## To transfer files to/from the CRAY with sftp 
##

sftp username@cray2.colostate.edu

##
## On a windows machine you can use PuTTy or FileZilla
##
## PuTTy is available at: http://www.chiark.greenend.org.uk/~sgtatham/putty/
##
## FileZilla is available at: http://filezilla-project.org/
##

##
## The CRAY is organized into modules that configure the shell for a particular
## application. The advantage of modules is that you are not required to
## specify explicit paths for different executable versions or to set the
## $MANPATH and other environment variables manually.
## 

module list  # view the module files currently laoded in your shell environment

module avail  # see a list of all module files on the CRAY

module load modulefile  # to load a modulefile

module unload modulefile  # unloads a loaded modulefile

module swap modulefile1 modulefile2 # to swap modulefile1 for modulefile2
                                    #e.g. module swap PrgEnv-cray PrgEnv-gnu
module show modulefile  # see the contents of a particular module

module help modulefile  # to see a description of the modulefile

##
## Interactive Jobs
##

# All parallel compute jobs must be run in the "lustrefs" directory

# Small jobs can be run on the Interactive Nodes to test code, etc.

# Large parallel jobs should be run on the Batch Compute Nodes 

##
## aprun Submits jobs for placement and execution
##

aprun -n 1 executable  # will run the executable file
                       # the -n 1 specifies the use of 1 core

aprun-n 24 executable  # run an executable file over 24 cores

apstat  # Shows all currently running applications

apkill apid  # Kills a curently running interactive job
             # apid is the application ID found by running the apstat command
             # you must be the owner of an application to kill it

##
## Batch Jobs
##

# Small queue - reserved for small jobs that can run in less than one hour
# Medium queue - reserved for jobs that run in less than one day
# Large queue - reserved for jobs that run in less than one week
# Priority queue - reserved for special uses <- conatact system admin

## Max number of jobs per user
# Small queue - 20
# Medium queue - 2
# Large queue - 1

qstat -Q  # List all available queues (brief)

qstat -Qf  # List all available queues (full)

qstat  # show the status of jobs in all queues

qstat -u jtipton  # show only the status of jobs that belong in your account

qsub filename  # Submit a job to the default batch queue where filename 
               # is the name of a file that contains batch queue commands

qdel jobid  # Delte a job from the batch queues where jobid is the job ID 
            # displayed by the qstat command

##
## Sample Batch Jobs
##

# To use the batch queues, you must create a text file (batch script) that 
# contains Torqu/PBS commands. You submit this file with the qsub command:

qsub filename

# Here is a sample batch script that you can use to run jobs on the Cray, or 
# use as a template for more complex script

  #!/bin/bash 
  #PBS –N jobname 
  #PBS –j oe 
  #PBS –l mppwidth=24 
  #PBS –l walltime=1:00:00 
  #PBS –q small 
  cd $PBS_O_WORKDIR 
  aprun –n 24 executable
# The first line specifies the shell environment to use for the batch job. The
# "#PBS" mneumonic represents a PBS directive. The “-N” directive renames the
# output files to whatever name you specify. The “-j oe” option indicates that
# standard output and standard error information should be combined in a single
# file. The “-l mppwidth” option specifies the number of cores to allocate to
# your job. The number of cores should not exceed 1,344, which is the maximum
# number of cores available in the largest batch queue. The “-l walltime”
# option specifies the maximum amount of time in hours, minutes and seconds
# that the job may run before exiting. The “-q small” option specifies which
# queue the job should be run in. There are four options: “small”, “medium”, 
# “large”, “ccm_queue”. If no queue is specified in the batch script, the 
# job will be routed to the “small” queue by default. The PBS_O_WORKDIR 
# environment variable is generated by Torque/PBS and contains the absolute 
# path to the directory from which you submitted your job. This is required 
# for Torque/PBS to find your executable files.

##
## CRAY supports either OpenMP or MPI to parallelize code
##